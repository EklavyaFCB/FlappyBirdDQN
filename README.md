# FlappyBirdDQN

In this project we attempt to develop models which are able to learn to play the game ‘Flappy Bird’, and ideally surpass human level scores by using Reinforce- ment Learning techniques. Specifically, we investigate two completely different approaches, tile coding and deep q-learning networks (DQNs), to develop an general overview of the problem and deeper understanding on reinforcement learning techniques. We go through their respective technicalities, and present conclusive results on experiments and hyper-parameter selection, in order to develop an optimal policy.

Flappy Bird is a mobile game developed in 2013 in which the goal is to navigate a bird through gaps between pairs of randomly generated pipes for as long as possible. The bird can die only if it touches any part of a pipe, or if it falls down to the ground, which it is doing on its own continuously. The player can keep the bird alive by making it ‘flap’, thus increasing its vertical position for a short time before it starts falling again. The score is the number of pipes the bird has successfully gone through. This simple game was proven to be surprisingly challenging for humans, as it is hard to successfully master the precise position and timing of the bird before going through a pipe. In the sections below, we attempt to implement two different models which can learn to play this game optimally.

The goal with this approach was to attempt to implement a method with a fundamentally different approach to tile coding function approximation. By employing Deep Q-Learning Network (DQN), we wished to showcase how computer vision and deep neural networks such as convolutional neural networks (CNNs) can be used in the context of reinforcement learning as well. Essentially, we implement a DQN which attempts to approximate the Q-function, rather than directly use the traditional Q-Learning method. We also wanted to employ ‘experience replay’ in our model, as summarised by Mnih et al. [2013], in their pseudocode. For this method, I chose to use PyGame’s Flappy Bird environment implementation (PyGame), which I watered-down to simplify the training process.
